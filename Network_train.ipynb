{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, the neural network from the paper is constructed and trained on a data subset 'Train_sub.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-fmm in c:\\users\\dahl\\anaconda3\\envs\\deeplearning\\lib\\site-packages (2019.1.30)\n",
      "Requirement already satisfied: numpy>=1.0.2 in c:\\users\\dahl\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from scikit-fmm) (1.19.1)\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "!pip install scikit-fmm\n",
    "\n",
    "import pandas as pd \n",
    "import scipy.ndimage\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler           \n",
    "from sklearn.model_selection import train_test_split        \n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib as jb\n",
    "from time import time\n",
    "import skfmm\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from .csv file and check the structure. 'head_diff' is the simulated head differences that we want to train the neural network to predict. These values are target data, while the remaining columns are used af input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           head_diff         head          dist           time     dist_well  \\\n",
      "0       1.872103e+00  1317.703197   1573.723027      76.418131    -88.388348   \n",
      "1       5.224641e-01  1294.126078   7605.546692      41.734163    -88.388348   \n",
      "2       4.676374e+00  1237.888684   3994.917952     307.432679    -88.388348   \n",
      "3       4.355244e+01  1180.976960    591.318456      74.482036    -88.388348   \n",
      "4       3.138261e+01  1282.674114   1122.243424     418.531152    -88.388348   \n",
      "...              ...          ...           ...            ...           ...   \n",
      "283279  1.245266e-06  1259.621139  12720.084221  174249.798917  85470.924736   \n",
      "283280  2.184873e-06  1282.942527  12864.142773  215469.315504  85897.565487   \n",
      "283281  1.444064e-06  1296.432742  14396.070860  223816.682425  86296.839399   \n",
      "283282  2.225960e-06  1298.854621  14735.019025  193333.001631  86687.341884   \n",
      "283283  6.783698e-07  1253.668232  12751.463566  217922.169676  87419.122736   \n",
      "\n",
      "         h_cond  h_cond_log  \n",
      "0       1.25000     0.09691  \n",
      "1       2.50000     0.39794  \n",
      "2       0.31250    -0.50515  \n",
      "3       0.00625    -2.20412  \n",
      "4       0.02500    -1.60206  \n",
      "...         ...         ...  \n",
      "283279  0.01250    -1.90309  \n",
      "283280  0.00250    -2.60206  \n",
      "283281  0.00250    -2.60206  \n",
      "283282  0.00250    -2.60206  \n",
      "283283  0.01250    -1.90309  \n",
      "\n",
      "[283284 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('Train_sub.csv'):\n",
    "    data_collection = pd.read_csv('Train_sub.csv')\n",
    "    \n",
    "if not os.path.exists('Train_sub.csv'):\n",
    "    url = 'https://github.com/MathiasBusk/HYDROsim-paper/blob/main/Train_sub.zip?raw=true'\n",
    "    Path = tf.keras.utils.get_file('Train_sub.zip', url)\n",
    "    zf = zipfile.ZipFile(Path)\n",
    "    data_collection = pd.read_csv(zf.open('Train_sub.csv'))\n",
    "    \n",
    "    \n",
    "print(data_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training, test, and validation data and scale the data using the StandardScaler() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test, y_train1, y_test = train_test_split(data_collection.iloc[:,1:7], data_collection.iloc[:,0], test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train1, y_train1, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network is constructed using tensorflow.keras layers. Input has 6 features that are connected to 3 hidden layers with 75 neurons each. We add a probabilistic output layer with tensorflow probability and predicts two output values - the mean and standard deviation of the output distribution. The optimizer is Adam and we try to minimize the negative log-likelihood loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputA (InputLayer)          [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 75)                525       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 75)                5700      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 75)                5700      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 152       \n",
      "_________________________________________________________________\n",
      "distribution_lambda_1 (Distr ((None, 1), (None, 1))    0         \n",
      "=================================================================\n",
      "Total params: 12,077\n",
      "Trainable params: 12,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
    "\n",
    "inputA = Input(shape=(6,),name='inputA')\n",
    "\n",
    "interpB = Dense(75, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.0001), activity_regularizer=tf.keras.regularizers.l2(0.0001))(inputA)\n",
    "interp1B = Dense(75, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.0001), activity_regularizer=tf.keras.regularizers.l2(0.0001))(interpB)\n",
    "interp2B = Dense(75, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.0001), activity_regularizer=tf.keras.regularizers.l2(0.0001))(interp1B)\n",
    "output = Dense(1+1, activation='linear')(interp2B)\n",
    "outputs =  tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[..., :1],scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:])))(output)\n",
    "\n",
    "model = Model(inputs=inputA, outputs=outputs)\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=20000,\n",
    "    decay_rate=0.95)\n",
    "model.compile(optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=lr_schedule), loss=negloglik)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203964 samples, validate on 56657 samples\n",
      "Epoch 1/300\n",
      "203964/203964 [==============================] - 3s 15us/sample - loss: 0.7091 - val_loss: -0.4053\n",
      "Epoch 2/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -0.4377 - val_loss: -0.8995\n",
      "Epoch 3/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -0.7797 - val_loss: -0.8507\n",
      "Epoch 4/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.1800 - val_loss: -1.0862\n",
      "Epoch 5/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.2085 - val_loss: -1.3108\n",
      "Epoch 6/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -0.9695 - val_loss: -1.4332\n",
      "Epoch 7/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.4877 - val_loss: -1.6442\n",
      "Epoch 8/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.5047 - val_loss: -1.6400\n",
      "Epoch 9/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.5541 - val_loss: -1.6156\n",
      "Epoch 10/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.6162 - val_loss: -1.7166\n",
      "Epoch 11/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.6490 - val_loss: -1.8323\n",
      "Epoch 12/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.6634 - val_loss: -1.8400\n",
      "Epoch 13/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.7885 - val_loss: -1.7959\n",
      "Epoch 14/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.6557 - val_loss: -1.6262\n",
      "Epoch 15/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.7632 - val_loss: -1.9492\n",
      "Epoch 16/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.9053 - val_loss: -2.0263\n",
      "Epoch 17/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.7369 - val_loss: -1.9014\n",
      "Epoch 18/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.8909 - val_loss: -2.0261\n",
      "Epoch 19/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.9407 - val_loss: -1.9882\n",
      "Epoch 20/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.9101 - val_loss: -2.0489\n",
      "Epoch 21/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -1.8517 - val_loss: -1.2383\n",
      "Epoch 22/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.9920 - val_loss: -2.1035\n",
      "Epoch 23/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.0662 - val_loss: -2.1631\n",
      "Epoch 24/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.0578 - val_loss: -2.2378\n",
      "Epoch 25/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.9139 - val_loss: -2.0986\n",
      "Epoch 26/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.0786 - val_loss: -1.9991\n",
      "Epoch 27/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1178 - val_loss: -1.9787\n",
      "Epoch 28/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1196 - val_loss: -2.2192\n",
      "Epoch 29/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.0776 - val_loss: -2.1668\n",
      "Epoch 30/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1663 - val_loss: -2.1761\n",
      "Epoch 31/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.1112 - val_loss: -2.1907\n",
      "Epoch 32/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1492 - val_loss: -2.2173\n",
      "Epoch 33/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.0427 - val_loss: -2.3333\n",
      "Epoch 34/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1967 - val_loss: -2.3085\n",
      "Epoch 35/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2094 - val_loss: -2.3714\n",
      "Epoch 36/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1489 - val_loss: -2.3733\n",
      "Epoch 37/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.1639 - val_loss: -2.3572\n",
      "Epoch 38/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2679 - val_loss: -2.3714\n",
      "Epoch 39/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2454 - val_loss: -2.3456\n",
      "Epoch 40/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.1895 - val_loss: -2.1754\n",
      "Epoch 41/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3244 - val_loss: -2.3797\n",
      "Epoch 42/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1827 - val_loss: -2.0773\n",
      "Epoch 43/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3166 - val_loss: -2.4137\n",
      "Epoch 44/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3115 - val_loss: -2.4019\n",
      "Epoch 45/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3316 - val_loss: -2.3692\n",
      "Epoch 46/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2935 - val_loss: -2.3629\n",
      "Epoch 47/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3472 - val_loss: -2.4489\n",
      "Epoch 48/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2911 - val_loss: -2.2725\n",
      "Epoch 49/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2774 - val_loss: -2.2284\n",
      "Epoch 50/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3535 - val_loss: -2.4129\n",
      "Epoch 51/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3754 - val_loss: -2.3524\n",
      "Epoch 52/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3812 - val_loss: -2.4257\n",
      "Epoch 53/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3294 - val_loss: -2.3584\n",
      "Epoch 54/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3526 - val_loss: -2.4249\n",
      "Epoch 55/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4059 - val_loss: -2.4943\n",
      "Epoch 56/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3617 - val_loss: -2.5010\n",
      "Epoch 57/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4057 - val_loss: -2.5251\n",
      "Epoch 58/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1313 - val_loss: -2.0733\n",
      "Epoch 59/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2989 - val_loss: -2.4269\n",
      "Epoch 60/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3703 - val_loss: -2.4929\n",
      "Epoch 61/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4001 - val_loss: -2.4958\n",
      "Epoch 62/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4288 - val_loss: -2.4818\n",
      "Epoch 63/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3483 - val_loss: -2.5409\n",
      "Epoch 64/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4563 - val_loss: -2.5619\n",
      "Epoch 65/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3460 - val_loss: -2.4686\n",
      "Epoch 66/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4690 - val_loss: -2.5753\n",
      "Epoch 67/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3979 - val_loss: -2.5334\n",
      "Epoch 68/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4283 - val_loss: -2.4948\n",
      "Epoch 69/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4904 - val_loss: -2.5187\n",
      "Epoch 70/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4924 - val_loss: -2.4484\n",
      "Epoch 71/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4991 - val_loss: -2.4651\n",
      "Epoch 72/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.4649 - val_loss: -2.5099\n",
      "Epoch 73/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4966 - val_loss: -0.3946\n",
      "Epoch 74/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4932 - val_loss: -2.5458\n",
      "Epoch 75/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4860 - val_loss: -2.4775\n",
      "Epoch 76/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4598 - val_loss: -2.5991\n",
      "Epoch 77/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5305 - val_loss: -2.5444\n",
      "Epoch 78/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5011 - val_loss: -2.5617\n",
      "Epoch 79/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.5462 - val_loss: -2.3656\n",
      "Epoch 80/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5395 - val_loss: -2.5243\n",
      "Epoch 81/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5681 - val_loss: -2.6488\n",
      "Epoch 82/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5422 - val_loss: -2.6089\n",
      "Epoch 83/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5629 - val_loss: -2.6167\n",
      "Epoch 84/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5660 - val_loss: -2.6204\n",
      "Epoch 85/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5731 - val_loss: -2.6228\n",
      "Epoch 86/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5592 - val_loss: -2.5222\n",
      "Epoch 87/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5882 - val_loss: -2.6565\n",
      "Epoch 88/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6185 - val_loss: -2.4593\n",
      "Epoch 89/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5930 - val_loss: -2.5247\n",
      "Epoch 90/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6000 - val_loss: -2.2028\n",
      "Epoch 91/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5774 - val_loss: -2.6011\n",
      "Epoch 92/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6029 - val_loss: -2.4857\n",
      "Epoch 93/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5671 - val_loss: -2.6062\n",
      "Epoch 94/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6118 - val_loss: -2.5370\n",
      "Epoch 95/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6151 - val_loss: -2.6123\n",
      "Epoch 96/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.6085 - val_loss: -2.6592\n",
      "Epoch 97/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6018 - val_loss: -2.7256\n",
      "Epoch 98/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6225 - val_loss: -2.6017\n",
      "Epoch 99/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6547 - val_loss: -2.5880\n",
      "Epoch 100/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.6452 - val_loss: -2.6575\n",
      "Epoch 101/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6467 - val_loss: -2.6922\n",
      "Epoch 102/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6334 - val_loss: -2.6879\n",
      "Epoch 103/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6421 - val_loss: -2.6022\n",
      "Epoch 104/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6629 - val_loss: -2.7291\n",
      "Epoch 105/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6685 - val_loss: -2.5174\n",
      "Epoch 106/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6546 - val_loss: -2.6970\n",
      "Epoch 107/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6755 - val_loss: -2.7070\n",
      "Epoch 108/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6716 - val_loss: -2.6730\n",
      "Epoch 109/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6727 - val_loss: -2.6816\n",
      "Epoch 110/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6770 - val_loss: -2.7371\n",
      "Epoch 111/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6899 - val_loss: -2.7329\n",
      "Epoch 112/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6951 - val_loss: -2.7291\n",
      "Epoch 113/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6490 - val_loss: -2.6991\n",
      "Epoch 114/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7098 - val_loss: -2.7293\n",
      "Epoch 115/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7047 - val_loss: -2.6468\n",
      "Epoch 116/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7039 - val_loss: -2.7267\n",
      "Epoch 117/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7219 - val_loss: -2.7358\n",
      "Epoch 118/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6588 - val_loss: -2.6317\n",
      "Epoch 119/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6921 - val_loss: -2.7688\n",
      "Epoch 120/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7002 - val_loss: -2.6719\n",
      "Epoch 121/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7181 - val_loss: -2.7063\n",
      "Epoch 122/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7004 - val_loss: -2.4876\n",
      "Epoch 123/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6914 - val_loss: -2.4667\n",
      "Epoch 124/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6309 - val_loss: -2.7251\n",
      "Epoch 125/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7268 - val_loss: -2.6739\n",
      "Epoch 126/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7337 - val_loss: -2.1625\n",
      "Epoch 127/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7268 - val_loss: -2.7396\n",
      "Epoch 128/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.7199 - val_loss: -2.7630\n",
      "Epoch 129/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7297 - val_loss: -2.4628\n",
      "Epoch 130/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7537 - val_loss: -2.7884\n",
      "Epoch 131/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7292 - val_loss: -2.6453\n",
      "Epoch 132/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7430 - val_loss: -2.7308\n",
      "Epoch 133/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7163 - val_loss: -2.6597\n",
      "Epoch 134/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7331 - val_loss: -2.7053\n",
      "Epoch 135/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.7616 - val_loss: -2.6883\n",
      "Epoch 136/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7249 - val_loss: -2.6750\n",
      "Epoch 137/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7107 - val_loss: -2.7513\n",
      "Epoch 138/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7521 - val_loss: -2.7466\n",
      "Epoch 139/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7581 - val_loss: -2.7921\n",
      "Epoch 140/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.7704 - val_loss: -2.7667\n",
      "Epoch 141/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.7446 - val_loss: -2.7896\n",
      "Epoch 142/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.7540 - val_loss: -2.7184\n",
      "Epoch 143/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7060 - val_loss: -2.6763\n",
      "Epoch 144/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7036 - val_loss: -2.7952\n",
      "Epoch 145/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7689 - val_loss: -2.8096\n",
      "Epoch 146/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7897 - val_loss: -2.7228\n",
      "Epoch 147/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7777 - val_loss: -2.8116\n",
      "Epoch 148/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7794 - val_loss: -2.7883\n",
      "Epoch 149/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7684 - val_loss: -2.8070\n",
      "Epoch 150/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7882 - val_loss: -2.7618\n",
      "Epoch 151/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7439 - val_loss: -2.7999\n",
      "Epoch 152/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7689 - val_loss: -2.7775\n",
      "Epoch 153/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7981 - val_loss: -2.7085\n",
      "Epoch 154/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7908 - val_loss: -2.8122\n",
      "Epoch 155/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.7616 - val_loss: -2.5218\n",
      "Epoch 156/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.7874 - val_loss: -2.7780\n",
      "Epoch 157/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7703 - val_loss: -2.7277\n",
      "Epoch 158/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7956 - val_loss: -2.7486\n",
      "Epoch 159/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7906 - val_loss: -2.8200\n",
      "Epoch 160/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7945 - val_loss: -2.7526\n",
      "Epoch 161/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7965 - val_loss: -2.7925\n",
      "Epoch 162/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8049 - val_loss: -2.7408\n",
      "Epoch 163/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7847 - val_loss: -2.7791\n",
      "Epoch 164/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7901 - val_loss: -2.8097\n",
      "Epoch 165/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8038 - val_loss: -2.8311\n",
      "Epoch 166/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7593 - val_loss: -2.7271\n",
      "Epoch 167/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7550 - val_loss: -2.5629\n",
      "Epoch 168/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7868 - val_loss: -2.7738\n",
      "Epoch 169/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8113 - val_loss: -2.8337\n",
      "Epoch 170/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8062 - val_loss: -2.8103\n",
      "Epoch 171/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8005 - val_loss: -2.8386\n",
      "Epoch 172/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7744 - val_loss: -2.6719\n",
      "Epoch 173/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7865 - val_loss: -2.8502\n",
      "Epoch 174/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8354 - val_loss: -2.8047\n",
      "Epoch 175/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8233 - val_loss: -2.7927\n",
      "Epoch 176/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8183 - val_loss: -2.7686\n",
      "Epoch 177/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8132 - val_loss: -2.8350\n",
      "Epoch 178/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8196 - val_loss: -2.8096\n",
      "Epoch 179/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8141 - val_loss: -2.8551\n",
      "Epoch 180/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8269 - val_loss: -2.8125\n",
      "Epoch 181/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8274 - val_loss: -2.7929\n",
      "Epoch 182/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8335 - val_loss: -2.7941\n",
      "Epoch 183/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8416 - val_loss: -2.8108\n",
      "Epoch 184/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8104 - val_loss: -2.7860\n",
      "Epoch 185/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8301 - val_loss: -2.6365\n",
      "Epoch 186/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8315 - val_loss: -2.8273\n",
      "Epoch 187/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8432 - val_loss: -2.8223\n",
      "Epoch 188/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8415 - val_loss: -2.8169\n",
      "Epoch 189/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8309 - val_loss: -2.6990\n",
      "Epoch 190/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8462 - val_loss: -2.8694\n",
      "Epoch 191/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8239 - val_loss: -2.7143\n",
      "Epoch 192/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8235 - val_loss: -2.8432\n",
      "Epoch 193/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8542 - val_loss: -2.7654\n",
      "Epoch 194/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8569 - val_loss: -2.8315\n",
      "Epoch 195/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8383 - val_loss: -2.8431\n",
      "Epoch 196/300\n",
      "203964/203964 [==============================] - 2s 11us/sample - loss: -2.8401 - val_loss: -2.7937\n",
      "Epoch 197/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8405 - val_loss: -2.5811\n",
      "Epoch 198/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.7890 - val_loss: -2.7407\n",
      "Epoch 199/300\n",
      "203964/203964 [==============================] - 2s 11us/sample - loss: -2.8376 - val_loss: -2.6542\n",
      "Epoch 200/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8647 - val_loss: -2.8720\n",
      "Epoch 201/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8617 - val_loss: -2.8256\n",
      "Epoch 202/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8722 - val_loss: -2.8833\n",
      "Epoch 203/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8508 - val_loss: -2.6225\n",
      "Epoch 204/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8577 - val_loss: -2.8058\n",
      "Epoch 205/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8537 - val_loss: -2.8420\n",
      "Epoch 206/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8501 - val_loss: -2.8764\n",
      "Epoch 207/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8642 - val_loss: -2.6355\n",
      "Epoch 208/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8611 - val_loss: -2.7931\n",
      "Epoch 209/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8754 - val_loss: -2.8573\n",
      "Epoch 210/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8507 - val_loss: -2.8213\n",
      "Epoch 211/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8813 - val_loss: -2.6790\n",
      "Epoch 212/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8756 - val_loss: -2.8826\n",
      "Epoch 213/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8547 - val_loss: -2.8462\n",
      "Epoch 214/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8792 - val_loss: -2.8584\n",
      "Epoch 215/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8437 - val_loss: -2.7229\n",
      "Epoch 216/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8646 - val_loss: -2.8558\n",
      "Epoch 217/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8867 - val_loss: -2.8852\n",
      "Epoch 218/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8513 - val_loss: -2.8623\n",
      "Epoch 219/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8819 - val_loss: -2.8468\n",
      "Epoch 220/300\n",
      "203964/203964 [==============================] - 2s 11us/sample - loss: -2.8805 - val_loss: -2.8592\n",
      "Epoch 221/300\n",
      "203964/203964 [==============================] - 2s 11us/sample - loss: -2.8587 - val_loss: -2.8402\n",
      "Epoch 222/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8394 - val_loss: -2.8279\n",
      "Epoch 223/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8627 - val_loss: -2.8753\n",
      "Epoch 224/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8744 - val_loss: -2.8014\n",
      "Epoch 225/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8564 - val_loss: -2.8606\n",
      "Epoch 226/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8439 - val_loss: -2.8304\n",
      "Epoch 227/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8842 - val_loss: -2.7876\n",
      "Epoch 228/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8707 - val_loss: -2.8647\n",
      "Epoch 229/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8908 - val_loss: -2.9007\n",
      "Epoch 230/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8319 - val_loss: -2.8472\n",
      "Epoch 231/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8876 - val_loss: -2.8843\n",
      "Epoch 232/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8763 - val_loss: -2.7879\n",
      "Epoch 233/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8755 - val_loss: -2.8449\n",
      "Epoch 234/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8939 - val_loss: -2.8643\n",
      "Epoch 235/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9046 - val_loss: -2.8598\n",
      "Epoch 236/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9056 - val_loss: -2.7757\n",
      "Epoch 237/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8935 - val_loss: -2.7157\n",
      "Epoch 238/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8897 - val_loss: -2.8584\n",
      "Epoch 239/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8541 - val_loss: -2.7020\n",
      "Epoch 240/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.7904 - val_loss: -2.8690\n",
      "Epoch 241/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8732 - val_loss: -2.8067\n",
      "Epoch 242/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8686 - val_loss: -2.8910\n",
      "Epoch 243/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8855 - val_loss: -2.8548\n",
      "Epoch 244/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8901 - val_loss: -2.8849\n",
      "Epoch 245/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9057 - val_loss: -2.8825\n",
      "Epoch 246/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.9046 - val_loss: -2.8659\n",
      "Epoch 247/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8871 - val_loss: -2.8902\n",
      "Epoch 248/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8941 - val_loss: -2.7483\n",
      "Epoch 249/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8713 - val_loss: -2.8095\n",
      "Epoch 250/300\n",
      "203964/203964 [==============================] - 2s 11us/sample - loss: -2.8686 - val_loss: -2.8511\n",
      "Epoch 251/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8856 - val_loss: -2.8308\n",
      "Epoch 252/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8974 - val_loss: -2.8626\n",
      "Epoch 253/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9005 - val_loss: -2.8858\n",
      "Epoch 254/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9123 - val_loss: -2.8342\n",
      "Epoch 255/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9054 - val_loss: -2.5720\n",
      "Epoch 256/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8905 - val_loss: -2.6283\n",
      "Epoch 257/300\n",
      "203964/203964 [==============================] - 2s 11us/sample - loss: -2.8795 - val_loss: -2.7787\n",
      "Epoch 258/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9035 - val_loss: -2.8914\n",
      "Epoch 259/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8924 - val_loss: -2.7974\n",
      "Epoch 260/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8690 - val_loss: -2.8455\n",
      "Epoch 261/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8860 - val_loss: -2.8397\n",
      "Epoch 262/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8954 - val_loss: -2.8747\n",
      "Epoch 263/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9100 - val_loss: -2.8001\n",
      "Epoch 264/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8271 - val_loss: -2.7267\n",
      "Epoch 265/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8524 - val_loss: -2.8891\n",
      "Epoch 266/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9079 - val_loss: -2.9117\n",
      "Epoch 267/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9134 - val_loss: -2.8774\n",
      "Epoch 268/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9304 - val_loss: -2.9208\n",
      "Epoch 269/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9220 - val_loss: -2.9067\n",
      "Epoch 270/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8803 - val_loss: -2.7569\n",
      "Epoch 271/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8446 - val_loss: -2.8469\n",
      "Epoch 272/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8859 - val_loss: -2.8844\n",
      "Epoch 273/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9087 - val_loss: -2.8364\n",
      "Epoch 274/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9200 - val_loss: -2.9014\n",
      "Epoch 275/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8761 - val_loss: -2.8853\n",
      "Epoch 276/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9287 - val_loss: -2.8757\n",
      "Epoch 277/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9199 - val_loss: -2.9212\n",
      "Epoch 278/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9336 - val_loss: -2.8960\n",
      "Epoch 279/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9348 - val_loss: -2.8747\n",
      "Epoch 280/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9023 - val_loss: -2.8541\n",
      "Epoch 281/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.7869 - val_loss: -2.8172\n",
      "Epoch 282/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8705 - val_loss: -2.8439\n",
      "Epoch 283/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9173 - val_loss: -2.9018\n",
      "Epoch 284/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8950 - val_loss: -2.8516\n",
      "Epoch 285/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9314 - val_loss: -2.9234\n",
      "Epoch 286/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9141 - val_loss: -2.9280\n",
      "Epoch 287/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9319 - val_loss: -2.9293\n",
      "Epoch 288/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9334 - val_loss: -2.8824\n",
      "Epoch 289/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8701 - val_loss: -2.7610\n",
      "Epoch 290/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8882 - val_loss: -2.8553\n",
      "Epoch 291/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9312 - val_loss: -2.9006\n",
      "Epoch 292/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8599 - val_loss: -2.8883\n",
      "Epoch 293/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8872 - val_loss: -2.8777\n",
      "Epoch 294/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9231 - val_loss: -2.9165\n",
      "Epoch 295/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9336 - val_loss: -2.9079\n",
      "Epoch 296/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9378 - val_loss: -2.8643\n",
      "Epoch 297/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.9321 - val_loss: -2.8590\n",
      "Epoch 298/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9053 - val_loss: -2.8510\n",
      "Epoch 299/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.8430 - val_loss: -2.8898\n",
      "Epoch 300/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9207 - val_loss: -2.9203\n"
     ]
    }
   ],
   "source": [
    "results=model.fit(X_train,y_train,batch_size=256,epochs=300,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the head change from the validation data set and visiualize the results compared to the goal values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "This took 0.06 seconds\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "t1 = time.time()\n",
    "X_val = scaler.transform(X_val)\n",
    "y_hat = model(X_val)\n",
    "mean = y_hat.mean()\n",
    "stddev = y_hat.stddev()\n",
    "t2 = time.time()\n",
    "print('This took {} seconds'.format(round(t2-t1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.9052\n",
      "RMSE: 0.9514\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "y_testi = y_val\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "ax.scatter(y_testi, mean,s=15)\n",
    "ax.plot([y_testi.min(), y_testi.max()], [y_testi.min(), y_testi.max()], 'k--', lw=4)\n",
    "#ax.set_xlim([-5,20])#\n",
    "#ax.set_ylim([-5,20])\n",
    "ax.set_xlabel('Measured head change [m]')\n",
    "ax.set_ylabel('Predicted head change [m]')\n",
    "#plt.show()\n",
    "fig.savefig('Validation')\n",
    "\n",
    "MSE = mean_squared_error(y_testi,mean) #Mean square of the residuals\n",
    "print(\"MSE: {}\" .format(round((MSE), 4))) #Root mean square error\n",
    "print(\"RMSE: {}\" .format(round(np.sqrt(MSE), 4))) #Root mean square error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network can be saved and loaded again for further use elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#model.save(\"Trained_network_sub.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feel free to play around with the model and investigate the effects of more epocs, different number of hidden layers, or number of neurons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
