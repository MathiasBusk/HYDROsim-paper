{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, the neural network from the paper is constructed and trained on a data subset 'Train_sub.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd \n",
    "import scipy.ndimage\n",
    "import scipy.signal\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler           \n",
    "from sklearn.model_selection import train_test_split        \n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib as jb\n",
    "from time import time\n",
    "import glob\n",
    "import skfmm\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from .csv file and check the structure. 'head_diff' is the simulated head differences that we want to train the neural network to predict. These values are target data, while the remaining columns are used af input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           head_diff         head          dist           time     dist_well  \\\n",
      "0       1.872103e+00  1317.703197   1573.723027      76.418131    -88.388348   \n",
      "1       5.224641e-01  1294.126078   7605.546692      41.734163    -88.388348   \n",
      "2       4.676374e+00  1237.888684   3994.917952     307.432679    -88.388348   \n",
      "3       4.355244e+01  1180.976960    591.318456      74.482036    -88.388348   \n",
      "4       3.138261e+01  1282.674114   1122.243424     418.531152    -88.388348   \n",
      "...              ...          ...           ...            ...           ...   \n",
      "283279  1.245266e-06  1259.621139  12720.084221  174249.798917  85470.924736   \n",
      "283280  2.184873e-06  1282.942527  12864.142773  215469.315504  85897.565487   \n",
      "283281  1.444064e-06  1296.432742  14396.070860  223816.682425  86296.839399   \n",
      "283282  2.225960e-06  1298.854621  14735.019025  193333.001631  86687.341884   \n",
      "283283  6.783698e-07  1253.668232  12751.463566  217922.169676  87419.122736   \n",
      "\n",
      "         h_cond  h_cond_log  \n",
      "0       1.25000     0.09691  \n",
      "1       2.50000     0.39794  \n",
      "2       0.31250    -0.50515  \n",
      "3       0.00625    -2.20412  \n",
      "4       0.02500    -1.60206  \n",
      "...         ...         ...  \n",
      "283279  0.01250    -1.90309  \n",
      "283280  0.00250    -2.60206  \n",
      "283281  0.00250    -2.60206  \n",
      "283282  0.00250    -2.60206  \n",
      "283283  0.01250    -1.90309  \n",
      "\n",
      "[283284 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#if os.path.exists('Train_sub.csv'):\n",
    " #   data_collection = pd.read_csv('Train_sub.csv')\n",
    "    \n",
    "if os.path.exists('Train_sub.csv'):\n",
    "    url = 'https://github.com/MathiasBusk/HYDROsim-paper/blob/main/Train_sub.zip?raw=true'\n",
    "    Path = tf.keras.utils.get_file('Train_sub.zip', url)\n",
    "    zf = zipfile.ZipFile(Path)\n",
    "    data_collection = pd.read_csv(zf.open('Train_sub.csv'))\n",
    "    \n",
    "    \n",
    "print(data_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training, test, and validation data and scale the data using the StandardScaler() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test, y_train1, y_test = train_test_split(data_collection.iloc[:,1:7], data_collection.iloc[:,0], test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train1, y_train1, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network is constructed using tensorflow.keras layers. Input has 6 features that are connected to 3 hidden layers with 75 neurons each. We add a probabilistic output layer with tensorflow probability and predicts two output values - the mean and standard deviation of the output distribution. The optimizer is Adam and we try to minimize the negative log-likelihood loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputA (InputLayer)          [(None, 6)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 75)                525       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 75)                5700      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 75)                5700      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 152       \n",
      "_________________________________________________________________\n",
      "distribution_lambda (Distrib ((None, 1), (None, 1))    0         \n",
      "=================================================================\n",
      "Total params: 12,077\n",
      "Trainable params: 12,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
    "\n",
    "inputA = Input(shape=(6,),name='inputA')\n",
    "\n",
    "interpB = Dense(75, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.0001), activity_regularizer=tf.keras.regularizers.l2(0.0001))(inputA)\n",
    "interp1B = Dense(75, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.0001), activity_regularizer=tf.keras.regularizers.l2(0.0001))(interpB)\n",
    "interp2B = Dense(75, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.0001), activity_regularizer=tf.keras.regularizers.l2(0.0001))(interp1B)\n",
    "output = Dense(1+1, activation='linear')(interp2B)\n",
    "outputs =  tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[..., :1],scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:])))(output)\n",
    "\n",
    "model = Model(inputs=inputA, outputs=outputs)\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=20000,\n",
    "    decay_rate=0.95)\n",
    "model.compile(optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=lr_schedule), loss=negloglik)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203964 samples, validate on 56657 samples\n",
      "Epoch 1/300\n",
      "203964/203964 [==============================] - 3s 15us/sample - loss: 0.6369 - val_loss: -0.5810\n",
      "Epoch 2/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -0.5450 - val_loss: -0.9355\n",
      "Epoch 3/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.0021 - val_loss: -0.8218\n",
      "Epoch 4/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.1845 - val_loss: -1.4096\n",
      "Epoch 5/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.3341 - val_loss: -1.1728\n",
      "Epoch 6/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.4925 - val_loss: -1.6030\n",
      "Epoch 7/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.4552 - val_loss: -1.3263\n",
      "Epoch 8/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.5594 - val_loss: -1.6472\n",
      "Epoch 9/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.5747 - val_loss: -1.5732\n",
      "Epoch 10/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.6878 - val_loss: -1.8071\n",
      "Epoch 11/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.6889 - val_loss: -1.8741\n",
      "Epoch 12/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.7080 - val_loss: -1.7919\n",
      "Epoch 13/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.7748 - val_loss: -1.4251\n",
      "Epoch 14/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.8479 - val_loss: -2.0032\n",
      "Epoch 15/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.7385 - val_loss: -1.9966\n",
      "Epoch 16/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.7531 - val_loss: -2.0017\n",
      "Epoch 17/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.8763 - val_loss: -1.5518\n",
      "Epoch 18/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.9299 - val_loss: -2.0972\n",
      "Epoch 19/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.8536 - val_loss: -2.1380\n",
      "Epoch 20/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.8633 - val_loss: -0.9566\n",
      "Epoch 21/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.9698 - val_loss: -1.5797\n",
      "Epoch 22/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.0523 - val_loss: -1.2997\n",
      "Epoch 23/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.9666 - val_loss: -2.0201\n",
      "Epoch 24/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.0843 - val_loss: -1.9996\n",
      "Epoch 25/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.0015 - val_loss: -2.2019\n",
      "Epoch 26/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.0733 - val_loss: -2.1459\n",
      "Epoch 27/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.0362 - val_loss: -2.1517\n",
      "Epoch 28/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.1465 - val_loss: -2.2640\n",
      "Epoch 29/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1258 - val_loss: -1.1566\n",
      "Epoch 30/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1165 - val_loss: -2.1712\n",
      "Epoch 31/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1557 - val_loss: -2.2563\n",
      "Epoch 32/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1001 - val_loss: -2.3071\n",
      "Epoch 33/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -1.9025 - val_loss: -2.1455\n",
      "Epoch 34/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.1956 - val_loss: -2.2482\n",
      "Epoch 35/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2093 - val_loss: -2.2101\n",
      "Epoch 36/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.0424 - val_loss: -2.1987\n",
      "Epoch 37/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2240 - val_loss: -2.3128\n",
      "Epoch 38/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2201 - val_loss: -2.3481\n",
      "Epoch 39/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.2579 - val_loss: -2.3311\n",
      "Epoch 40/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2620 - val_loss: -2.2485\n",
      "Epoch 41/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2614 - val_loss: -2.3335\n",
      "Epoch 42/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2966 - val_loss: -2.3401\n",
      "Epoch 43/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.2869 - val_loss: -2.2273\n",
      "Epoch 44/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1356 - val_loss: -2.3518\n",
      "Epoch 45/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2621 - val_loss: -2.1530\n",
      "Epoch 46/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3183 - val_loss: -2.4343\n",
      "Epoch 47/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.1899 - val_loss: -2.3155\n",
      "Epoch 48/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.3331 - val_loss: -2.4245\n",
      "Epoch 49/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3326 - val_loss: -2.4164\n",
      "Epoch 50/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3095 - val_loss: -2.4571\n",
      "Epoch 51/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3101 - val_loss: -2.4521\n",
      "Epoch 52/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3394 - val_loss: -2.3702\n",
      "Epoch 53/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3358 - val_loss: -1.9658\n",
      "Epoch 54/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3493 - val_loss: -2.1594\n",
      "Epoch 55/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.4181 - val_loss: -2.5192\n",
      "Epoch 56/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.3502 - val_loss: -2.4241\n",
      "Epoch 57/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.3122 - val_loss: -2.0843\n",
      "Epoch 58/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4184 - val_loss: -2.5209\n",
      "Epoch 59/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.2183 - val_loss: -2.4623\n",
      "Epoch 60/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4331 - val_loss: -2.4541\n",
      "Epoch 61/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3730 - val_loss: -2.5219\n",
      "Epoch 62/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4325 - val_loss: -2.4823\n",
      "Epoch 63/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.3637 - val_loss: -2.4008\n",
      "Epoch 64/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4604 - val_loss: -1.8031\n",
      "Epoch 65/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.4425 - val_loss: -2.4253\n",
      "Epoch 66/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4624 - val_loss: -2.5574\n",
      "Epoch 67/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5000 - val_loss: -2.5727\n",
      "Epoch 68/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4733 - val_loss: -2.5566\n",
      "Epoch 69/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4534 - val_loss: -2.5637\n",
      "Epoch 70/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.4841 - val_loss: -2.5236\n",
      "Epoch 71/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5138 - val_loss: -2.5584\n",
      "Epoch 72/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5048 - val_loss: -2.5415\n",
      "Epoch 73/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5303 - val_loss: -2.4948\n",
      "Epoch 74/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5309 - val_loss: -2.6102\n",
      "Epoch 75/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5340 - val_loss: -2.6396\n",
      "Epoch 76/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5468 - val_loss: -1.9482\n",
      "Epoch 77/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5577 - val_loss: -2.5564\n",
      "Epoch 78/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5276 - val_loss: -2.6102\n",
      "Epoch 79/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5586 - val_loss: -2.6484\n",
      "Epoch 80/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5483 - val_loss: -2.3578\n",
      "Epoch 81/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5591 - val_loss: -2.4748\n",
      "Epoch 82/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5202 - val_loss: -2.4031\n",
      "Epoch 83/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5516 - val_loss: -2.6529\n",
      "Epoch 84/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5885 - val_loss: -2.6456\n",
      "Epoch 85/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5710 - val_loss: -2.5189\n",
      "Epoch 86/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6053 - val_loss: -2.6719\n",
      "Epoch 87/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5934 - val_loss: -2.6582\n",
      "Epoch 88/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5826 - val_loss: -2.3441\n",
      "Epoch 89/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5806 - val_loss: -2.6413\n",
      "Epoch 90/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6316 - val_loss: -2.6583\n",
      "Epoch 91/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.5949 - val_loss: -2.6678\n",
      "Epoch 92/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6442 - val_loss: -2.6254\n",
      "Epoch 93/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6263 - val_loss: -2.6364\n",
      "Epoch 94/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6397 - val_loss: -2.6323\n",
      "Epoch 95/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6292 - val_loss: -2.5638\n",
      "Epoch 96/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.5937 - val_loss: -2.6483\n",
      "Epoch 97/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6365 - val_loss: -2.4858\n",
      "Epoch 98/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6467 - val_loss: -2.7067\n",
      "Epoch 99/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6193 - val_loss: -2.6991\n",
      "Epoch 100/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6492 - val_loss: -2.5965\n",
      "Epoch 101/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6211 - val_loss: -2.6597\n",
      "Epoch 102/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6798 - val_loss: -2.7118\n",
      "Epoch 103/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6780 - val_loss: -2.7058\n",
      "Epoch 104/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6661 - val_loss: -2.7076\n",
      "Epoch 105/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.5804 - val_loss: -2.4781\n",
      "Epoch 106/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6677 - val_loss: -2.7313\n",
      "Epoch 107/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6663 - val_loss: -2.5337\n",
      "Epoch 108/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6176 - val_loss: -2.4086\n",
      "Epoch 109/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6170 - val_loss: -2.6568\n",
      "Epoch 110/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6727 - val_loss: -2.6888\n",
      "Epoch 111/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6911 - val_loss: -2.5436\n",
      "Epoch 112/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6764 - val_loss: -2.7400\n",
      "Epoch 113/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7080 - val_loss: -2.7449\n",
      "Epoch 114/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6632 - val_loss: -2.6191\n",
      "Epoch 115/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7191 - val_loss: -2.6619\n",
      "Epoch 116/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7260 - val_loss: -2.7250\n",
      "Epoch 117/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6823 - val_loss: -2.4497\n",
      "Epoch 118/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7193 - val_loss: -2.7467\n",
      "Epoch 119/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6090 - val_loss: -2.6856\n",
      "Epoch 120/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.6707 - val_loss: -2.5725\n",
      "Epoch 121/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6894 - val_loss: -2.7066\n",
      "Epoch 122/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7171 - val_loss: -2.7662\n",
      "Epoch 123/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7207 - val_loss: -2.7720\n",
      "Epoch 124/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7218 - val_loss: -2.7513\n",
      "Epoch 125/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7131 - val_loss: -2.6558\n",
      "Epoch 126/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7235 - val_loss: -2.7480\n",
      "Epoch 127/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6926 - val_loss: -1.7191\n",
      "Epoch 128/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7050 - val_loss: -2.7337\n",
      "Epoch 129/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7376 - val_loss: -2.6695\n",
      "Epoch 130/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7420 - val_loss: -2.6151\n",
      "Epoch 131/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7422 - val_loss: -2.7021\n",
      "Epoch 132/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7316 - val_loss: -2.7591\n",
      "Epoch 133/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7427 - val_loss: -2.7769\n",
      "Epoch 134/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7419 - val_loss: -2.7293\n",
      "Epoch 135/300\n",
      "203964/203964 [==============================] - 2s 8us/sample - loss: -2.7234 - val_loss: -2.7249\n",
      "Epoch 136/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7367 - val_loss: -2.6693\n",
      "Epoch 137/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7010 - val_loss: -2.5595\n",
      "Epoch 138/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7369 - val_loss: -2.7759\n",
      "Epoch 139/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7644 - val_loss: -2.7604\n",
      "Epoch 140/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7598 - val_loss: -2.7889\n",
      "Epoch 141/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7651 - val_loss: -2.7367\n",
      "Epoch 142/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7144 - val_loss: -2.6923\n",
      "Epoch 143/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7266 - val_loss: -2.7792\n",
      "Epoch 144/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7768 - val_loss: -2.8115\n",
      "Epoch 145/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6501 - val_loss: -2.7316\n",
      "Epoch 146/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7480 - val_loss: -2.7552\n",
      "Epoch 147/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7688 - val_loss: -2.7763\n",
      "Epoch 148/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7548 - val_loss: -2.7809\n",
      "Epoch 149/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7919 - val_loss: -2.8406\n",
      "Epoch 150/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7885 - val_loss: -2.7861\n",
      "Epoch 151/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7202 - val_loss: -2.7025\n",
      "Epoch 152/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7011 - val_loss: -2.7609\n",
      "Epoch 153/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7225 - val_loss: -2.7695\n",
      "Epoch 154/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7147 - val_loss: -2.7432\n",
      "Epoch 155/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7733 - val_loss: -2.7875\n",
      "Epoch 156/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7850 - val_loss: -2.7689\n",
      "Epoch 157/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8096 - val_loss: -2.7308\n",
      "Epoch 158/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7947 - val_loss: -2.8214\n",
      "Epoch 159/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7346 - val_loss: -2.6665\n",
      "Epoch 160/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7738 - val_loss: -2.7063\n",
      "Epoch 161/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7974 - val_loss: -2.8256\n",
      "Epoch 162/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8176 - val_loss: -2.8344\n",
      "Epoch 163/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8144 - val_loss: -2.7443\n",
      "Epoch 164/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6856 - val_loss: -2.6738\n",
      "Epoch 165/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7233 - val_loss: -2.5212\n",
      "Epoch 166/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7688 - val_loss: -2.7608\n",
      "Epoch 167/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7703 - val_loss: -2.7621\n",
      "Epoch 168/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8017 - val_loss: -2.8070\n",
      "Epoch 169/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8179 - val_loss: -2.7403\n",
      "Epoch 170/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8123 - val_loss: -2.7874\n",
      "Epoch 171/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7738 - val_loss: -2.7907\n",
      "Epoch 172/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8154 - val_loss: -2.7685\n",
      "Epoch 173/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6916 - val_loss: -2.6390\n",
      "Epoch 174/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7148 - val_loss: -2.7705\n",
      "Epoch 175/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7928 - val_loss: -2.7829\n",
      "Epoch 176/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8295 - val_loss: -2.7426\n",
      "Epoch 177/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7318 - val_loss: -2.7960\n",
      "Epoch 178/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8090 - val_loss: -2.8285\n",
      "Epoch 179/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8425 - val_loss: -2.8009\n",
      "Epoch 180/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8407 - val_loss: -2.7739\n",
      "Epoch 181/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8425 - val_loss: -2.8662\n",
      "Epoch 182/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8199 - val_loss: -2.7801\n",
      "Epoch 183/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7668 - val_loss: -2.7979\n",
      "Epoch 184/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8074 - val_loss: -2.8154\n",
      "Epoch 185/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8434 - val_loss: -2.8571\n",
      "Epoch 186/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8241 - val_loss: -2.8520\n",
      "Epoch 187/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7768 - val_loss: -2.7655\n",
      "Epoch 188/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8109 - val_loss: -2.7265\n",
      "Epoch 189/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7884 - val_loss: -2.8189\n",
      "Epoch 190/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8548 - val_loss: -2.8462\n",
      "Epoch 191/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8419 - val_loss: -2.8492\n",
      "Epoch 192/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8419 - val_loss: -2.7612\n",
      "Epoch 193/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8357 - val_loss: -2.8125\n",
      "Epoch 194/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.6712 - val_loss: -2.6745\n",
      "Epoch 195/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7494 - val_loss: -2.7744\n",
      "Epoch 196/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7613 - val_loss: -2.8092\n",
      "Epoch 197/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8199 - val_loss: -2.8287\n",
      "Epoch 198/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8400 - val_loss: -2.8294\n",
      "Epoch 199/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8507 - val_loss: -2.8569\n",
      "Epoch 200/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8513 - val_loss: -2.8648\n",
      "Epoch 201/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7870 - val_loss: -2.7556\n",
      "Epoch 202/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7314 - val_loss: -2.7002\n",
      "Epoch 203/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7600 - val_loss: -2.7788\n",
      "Epoch 204/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8119 - val_loss: -2.8363\n",
      "Epoch 205/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8363 - val_loss: -2.8515\n",
      "Epoch 206/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8215 - val_loss: -2.8293\n",
      "Epoch 207/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8608 - val_loss: -2.8682\n",
      "Epoch 208/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8413 - val_loss: -2.8565\n",
      "Epoch 209/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8560 - val_loss: -2.8571\n",
      "Epoch 210/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8026 - val_loss: -2.7537\n",
      "Epoch 211/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8608 - val_loss: -2.8889\n",
      "Epoch 212/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7854 - val_loss: -2.7521\n",
      "Epoch 213/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8686 - val_loss: -2.8355\n",
      "Epoch 214/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7954 - val_loss: -2.6970\n",
      "Epoch 215/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8138 - val_loss: -2.8463\n",
      "Epoch 216/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8523 - val_loss: -2.8623\n",
      "Epoch 217/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8833 - val_loss: -2.8774\n",
      "Epoch 218/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7386 - val_loss: -2.7597\n",
      "Epoch 219/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8120 - val_loss: -2.8146\n",
      "Epoch 220/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7356 - val_loss: -2.7869\n",
      "Epoch 221/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8429 - val_loss: -2.7399\n",
      "Epoch 222/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8495 - val_loss: -2.8220\n",
      "Epoch 223/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8786 - val_loss: -2.8434\n",
      "Epoch 224/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8148 - val_loss: -2.7892\n",
      "Epoch 225/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8478 - val_loss: -2.8166\n",
      "Epoch 226/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7929 - val_loss: -2.6303\n",
      "Epoch 227/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7575 - val_loss: -2.7261\n",
      "Epoch 228/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8300 - val_loss: -2.7471\n",
      "Epoch 229/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8565 - val_loss: -2.8192\n",
      "Epoch 230/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8803 - val_loss: -2.8667\n",
      "Epoch 231/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8717 - val_loss: -2.8229\n",
      "Epoch 232/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8708 - val_loss: -2.8489\n",
      "Epoch 233/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8945 - val_loss: -2.8504\n",
      "Epoch 234/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8595 - val_loss: -2.8629\n",
      "Epoch 235/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8311 - val_loss: -2.8697\n",
      "Epoch 236/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8533 - val_loss: -2.6018\n",
      "Epoch 237/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7935 - val_loss: -2.7888\n",
      "Epoch 238/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8619 - val_loss: -2.8847\n",
      "Epoch 239/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8764 - val_loss: -2.8465\n",
      "Epoch 240/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8945 - val_loss: -2.8390\n",
      "Epoch 241/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7792 - val_loss: -2.7841\n",
      "Epoch 242/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8253 - val_loss: -2.7627\n",
      "Epoch 243/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8707 - val_loss: -2.5966\n",
      "Epoch 244/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8817 - val_loss: -2.8788\n",
      "Epoch 245/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8784 - val_loss: -2.8445\n",
      "Epoch 246/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8767 - val_loss: -2.8848\n",
      "Epoch 247/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.9146 - val_loss: -2.8582\n",
      "Epoch 248/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8356 - val_loss: -2.7036\n",
      "Epoch 249/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8505 - val_loss: -2.8035\n",
      "Epoch 250/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8966 - val_loss: -2.8561\n",
      "Epoch 251/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9088 - val_loss: -2.8462\n",
      "Epoch 252/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9123 - val_loss: -2.8712\n",
      "Epoch 253/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8520 - val_loss: -2.8352\n",
      "Epoch 254/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7921 - val_loss: -2.7847\n",
      "Epoch 255/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8780 - val_loss: -2.8461\n",
      "Epoch 256/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8818 - val_loss: -2.8482\n",
      "Epoch 257/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8838 - val_loss: -2.6281\n",
      "Epoch 258/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8988 - val_loss: -2.8839\n",
      "Epoch 259/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9273 - val_loss: -2.7972\n",
      "Epoch 260/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.9038 - val_loss: -2.8535\n",
      "Epoch 261/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9080 - val_loss: -2.8471\n",
      "Epoch 262/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.9068 - val_loss: -2.8940\n",
      "Epoch 263/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9221 - val_loss: -2.8929\n",
      "Epoch 264/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9102 - val_loss: -2.8806\n",
      "Epoch 265/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8470 - val_loss: -2.7088\n",
      "Epoch 266/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8180 - val_loss: -2.8709\n",
      "Epoch 267/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.8323 - val_loss: -2.8251\n",
      "Epoch 268/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8841 - val_loss: -2.8854\n",
      "Epoch 269/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9168 - val_loss: -2.8955\n",
      "Epoch 270/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8610 - val_loss: -2.8224\n",
      "Epoch 271/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8976 - val_loss: -2.8584\n",
      "Epoch 272/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9343 - val_loss: -2.8652\n",
      "Epoch 273/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9264 - val_loss: -2.8964\n",
      "Epoch 274/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8902 - val_loss: -2.8462\n",
      "Epoch 275/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.9314 - val_loss: -2.8594\n",
      "Epoch 276/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8615 - val_loss: -2.8055\n",
      "Epoch 277/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8948 - val_loss: -2.7689\n",
      "Epoch 278/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9095 - val_loss: -2.8788\n",
      "Epoch 279/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9256 - val_loss: -2.8753\n",
      "Epoch 280/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9099 - val_loss: -2.9142\n",
      "Epoch 281/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9366 - val_loss: -2.9321\n",
      "Epoch 282/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9323 - val_loss: -2.8492\n",
      "Epoch 283/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9264 - val_loss: -2.8425\n",
      "Epoch 284/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9134 - val_loss: -2.8599\n",
      "Epoch 285/300\n",
      "203964/203964 [==============================] - 2s 10us/sample - loss: -2.7075 - val_loss: -2.7344\n",
      "Epoch 286/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7920 - val_loss: -2.8093\n",
      "Epoch 287/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8339 - val_loss: -2.7797\n",
      "Epoch 288/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8548 - val_loss: -2.7853\n",
      "Epoch 289/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8733 - val_loss: -2.7942\n",
      "Epoch 290/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8782 - val_loss: -2.8423\n",
      "Epoch 291/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8989 - val_loss: -2.8832\n",
      "Epoch 292/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9250 - val_loss: -2.8917\n",
      "Epoch 293/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8838 - val_loss: -2.7062\n",
      "Epoch 294/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.7620 - val_loss: -2.7892\n",
      "Epoch 295/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8692 - val_loss: -2.8125\n",
      "Epoch 296/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9075 - val_loss: -2.8677\n",
      "Epoch 297/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9146 - val_loss: -2.9013\n",
      "Epoch 298/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9361 - val_loss: -2.9030\n",
      "Epoch 299/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.8947 - val_loss: -2.8892\n",
      "Epoch 300/300\n",
      "203964/203964 [==============================] - 2s 9us/sample - loss: -2.9383 - val_loss: -2.9119\n"
     ]
    }
   ],
   "source": [
    "results=model.fit(X_train,y_train,batch_size=256,epochs=300,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the head change from the validation data set and visiualize the results compared to the goal values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "This took 0.04 seconds\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "t1 = time.time()\n",
    "X_val = scaler.transform(X_val)\n",
    "y_hat = model(X_val)\n",
    "mean = y_hat.mean()\n",
    "stddev = y_hat.stddev()\n",
    "t2 = time.time()\n",
    "print('This took {} seconds'.format(round(t2-t1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.0104\n",
      "RMSE: 1.0052\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "y_testi = y_val\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "ax.scatter(y_testi, mean,s=15)\n",
    "ax.plot([y_testi.min(), y_testi.max()], [y_testi.min(), y_testi.max()], 'k--', lw=4)\n",
    "#ax.set_xlim([-5,20])#\n",
    "#ax.set_ylim([-5,20])\n",
    "ax.set_xlabel('Measured head change [m]')\n",
    "ax.set_ylabel('Predicted head change [m]')\n",
    "#plt.show()\n",
    "fig.savefig('Validation')\n",
    "\n",
    "MSE = mean_squared_error(y_testi,mean) #Mean square of the residuals\n",
    "print(\"MSE: {}\" .format(round((MSE), 4))) #Root mean square error\n",
    "print(\"RMSE: {}\" .format(round(np.sqrt(MSE), 4))) #Root mean square error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network can be saved and loaded again for further use elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#model.save(\"Trained_network_sub.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feel free to play around with the model and investigate the effects of more epocs, different number of hidden layers, or number of neurons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
